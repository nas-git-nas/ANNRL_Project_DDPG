{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Information\n",
    "* Authors: Daniil Bobrovskiy and Nicolaj Schmid\n",
    "* Course: Artificial Neural Networks and Reinforcement Learning (CS-456, EPFL)\n",
    "* Project: DDPG implementation to balance an inverted pendulum\n",
    "* References: \n",
    "    * Code: based on OpenAI's _Gym_ environment\n",
    "    * Algorithm: _Continuous control with deep reinforcement learning_ from Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver and Daan Wierstra, 2019\n",
    "\n",
    "\n",
    "### Notebook Structure\n",
    "This notebook is structured in three parts:\n",
    "1. __Imports__: import all libraries\n",
    "2. __Classes__: DDPG implementation split in different classes\n",
    "3. __Training and Results__: run the algorithms and show the results\n",
    "    * Heuristic Policy\n",
    "    * Q-function\n",
    "    * Minimal Implementation of DDPG\n",
    "    * Target DDPG\n",
    "    * Ornstein-Uhlenbeck Noise\n",
    "    \n",
    "Please read the report for the discussion of the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from abc import abstractmethod\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, seed):     \n",
    "        self._buffer = { \n",
    "            \"state\" : torch.empty((buffer_size,3), dtype=torch.float32, requires_grad=False),\n",
    "            \"action\" : torch.empty((buffer_size), dtype=torch.float32, requires_grad=False),\n",
    "            \"reward\" : torch.empty((buffer_size), dtype=torch.float32, requires_grad=False),\n",
    "            \"next_state\" : torch.empty((buffer_size,3), dtype=torch.float32, requires_grad=False),\n",
    "            \"trunc\" : torch.empty((buffer_size), dtype=torch.bool, requires_grad=False)\n",
    "        }\n",
    "        self._rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        self._buffer_size = buffer_size\n",
    "        self._idx = 0\n",
    "        self._full = False\n",
    "\n",
    "    def addTransition(self, state, action ,reward, next_state, trunc):\n",
    "        # if len(self._buffer) >= self._buffer_size:\n",
    "        #     self._buffer.pop(0)\n",
    "        # self._buffer.append((state, action, reward, next_state, trunc))\n",
    "        self._buffer[\"state\"][self._idx,:] = self.numpy2tensor(state, shape_type=\"state\")\n",
    "        self._buffer[\"action\"][self._idx] = self.numpy2tensor(action, shape_type=\"action\")\n",
    "        self._buffer[\"reward\"][self._idx] = self.numpy2tensor(reward, shape_type=\"reward\")\n",
    "        self._buffer[\"next_state\"][self._idx,:] = self.numpy2tensor(next_state, shape_type=\"next_state\")\n",
    "        self._buffer[\"trunc\"][self._idx] = self.numpy2tensor(trunc, shape_type=\"trunc\")\n",
    "\n",
    "        # increment index\n",
    "        self._idx = self._idx + 1\n",
    "        if self._idx >= self._buffer_size:\n",
    "            self._idx = 0\n",
    "            self._full = True\n",
    "\n",
    "    def sampleBatch(self, batch_size):\n",
    "        \n",
    "        # determine current buffer size\n",
    "        if self._full:\n",
    "            current_buffer_size = self._buffer_size\n",
    "        else:\n",
    "            current_buffer_size = self._idx\n",
    "\n",
    "        # return None if batch_size is larger than current buffer size\n",
    "        if batch_size > current_buffer_size:\n",
    "            return None  \n",
    "\n",
    "        # choose random samples\n",
    "        rand_idx = self._rng.choice(current_buffer_size, size=batch_size, replace=False) # TODO: should the samples be removed ???\n",
    "        batch = { \n",
    "            \"state\" : self._buffer[\"state\"][rand_idx,:],\n",
    "            \"action\" : self._buffer[\"action\"][rand_idx],\n",
    "            \"reward\" : self._buffer[\"reward\"][rand_idx],\n",
    "            \"next_state\" : self._buffer[\"next_state\"][rand_idx,:],\n",
    "            \"trunc\" : self._buffer[\"trunc\"][rand_idx]\n",
    "        }\n",
    "        return batch\n",
    "    \n",
    "    def numpy2tensor(self, array, shape_type):\n",
    "        # convert numpy array to tensor if necessary\n",
    "        if not torch.is_tensor(array):\n",
    "            tensor = torch.tensor(array, dtype=torch.float32, requires_grad=False)\n",
    "        else:\n",
    "            tensor = array\n",
    "\n",
    "        # reshape tensor\n",
    "        if shape_type == \"state\":\n",
    "            tensor = tensor.reshape((-1,3))\n",
    "        elif shape_type == \"action\":\n",
    "            tensor = tensor.reshape((-1,1))\n",
    "        elif shape_type == \"reward\":\n",
    "            tensor = tensor.reshape((-1,1))\n",
    "        elif shape_type == \"next_state\":\n",
    "            tensor = tensor.reshape((-1,3))\n",
    "        elif shape_type == \"trunc\":\n",
    "            tensor = tensor.reshape((-1,1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid shape_type: {}\".format(shape_type))\n",
    "        \n",
    "        return tensor\n",
    "        \n",
    "    def detachClone(self, batch):\n",
    "        if batch is not None:\n",
    "            batch = { \n",
    "                \"state\" : batch[\"state\"].clone().detach(),\n",
    "                \"action\" : batch[\"action\"].clone().detach(),\n",
    "                \"reward\" : batch[\"reward\"].clone().detach(),\n",
    "                \"next_state\" : batch[\"next_state\"].clone().detach(),\n",
    "                \"trunc\" : batch[\"trunc\"].clone().detach()\n",
    "            }\n",
    "        return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNoise():\n",
    "    def __init__(self, sigma, seed):\n",
    "        self._sigma = sigma\n",
    "        self.theta = 1.0\n",
    "        self.generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    @abstractmethod\n",
    "    def getNoisyAction(self, actions):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "class GaussianActionNoise(ActionNoise):\n",
    "    def __init__(self, sigma, seed=0) -> None:\n",
    "        super().__init__(sigma, seed)\n",
    "\n",
    "    def getNoisyAction(self, actions):\n",
    "        noisy_action = actions + torch.normal(torch.zeros_like(actions), torch.ones_like(actions)*self._sigma, generator=self.generator)\n",
    "        return torch.clip(noisy_action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class OUActionNoise(ActionNoise):\n",
    "    def __init__(self, sigma, theta, seed=0):\n",
    "        super().__init__(sigma, seed)\n",
    "\n",
    "        self.theta = theta\n",
    "        self.prev_noise = 0.0\n",
    "\n",
    "    def getNoisyAction(self, actions):\n",
    "        noise  = (1-self.theta)*self.prev_noise + torch.normal(torch.zeros_like(actions), torch.ones_like(actions)*self._sigma, generator=self.generator)\n",
    "        noisy_action = actions + noise\n",
    "\n",
    "        self.prev_noise = noise\n",
    "        return torch.clip(noisy_action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.prev_noise = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(4, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        x = torch.cat((states, actions.reshape(-1,1)), dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class Critic():\n",
    "    def __init__(self, gamma, lr, tau):\n",
    "        # hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "       \n",
    "        # initialize critic network\n",
    "        self.critic_net = CriticNetwork()\n",
    "        self.actor = None\n",
    "\n",
    "        # intialize critic\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_net.parameters(), lr=self.lr)\n",
    "\n",
    "        # initialize target network\n",
    "        self.critic_target_net = CriticNetwork()\n",
    "\n",
    "        # logging values\n",
    "        self.log_losses = []\n",
    "\n",
    "    def saveModels(self, path):\n",
    "        torch.save(self.critic_net, os.path.join(path, \"critic_net.pt\"))\n",
    "        torch.save(self.critic_target_net, os.path.join(path, \"critic_target_net.pt\"))\n",
    "\n",
    "    def computeQValues(self, states, actions, target=False):\n",
    "        if target:\n",
    "            return self.critic_target_net.forward(states=states, actions=actions)\n",
    "        else:\n",
    "            return self.critic_net.forward(states=states, actions=actions)\n",
    "\n",
    "    def trainStep(self, batch, actor):\n",
    "        # do not train if relay buffer is not large enough\n",
    "        if batch is None:\n",
    "            self.log_losses.append(0)\n",
    "            return                \n",
    "\n",
    "        # gradient descent step for critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = self._computeCriticsLoss(batch=batch, actor=actor)\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # log loss\n",
    "        self.log_losses.append(critic_loss.item())\n",
    "\n",
    "        # update target network\n",
    "        self._updateTargetNetworks()\n",
    "\n",
    "    def _computeCriticsLoss(self, batch, actor):\n",
    "        # calculate next actions and target, with torch.no_grad()\n",
    "        with torch.no_grad():\n",
    "            # calculate next actions and next Q values\n",
    "            if self.tau == 1.0:\n",
    "                next_actions = actor.computeActions(states=batch[\"next_state\"], target=False, deterministic=True)\n",
    "                next_q_values = self.computeQValues(states=batch[\"next_state\"], actions=next_actions, target=False)\n",
    "            else:\n",
    "                next_actions = actor.computeActions(batch[\"next_state\"], target=True, deterministic=True)\n",
    "                next_q_values = self.computeQValues(states=batch[\"next_state\"], actions=next_actions, target=True)\n",
    "\n",
    "            # set next Q values to 0 if episode is truncated              \n",
    "            next_q_values = torch.where(batch[\"trunc\"].reshape_as(next_q_values), 0, next_q_values)\n",
    "            \n",
    "            # calculate target\n",
    "            targets = batch[\"reward\"].reshape_as(next_q_values) + self.gamma * next_q_values\n",
    "\n",
    "        # calculate target and expected cumulative rewards\n",
    "        q_values = self.computeQValues(states=batch[\"state\"], actions=batch[\"action\"], target=False)\n",
    "        \n",
    "        # calculate loss and log it\n",
    "        return 0.5 * torch.pow(q_values - targets, 2).mean()\n",
    "    \n",
    "    def _updateTargetNetworks(self):\n",
    "        if self.tau == 1.0:\n",
    "            return\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # update critic target network         \n",
    "            critic_net_dict = self.critic_net.state_dict()\n",
    "            critic_target_net_dict = self.critic_target_net.state_dict()\n",
    "            for key in critic_target_net_dict:\n",
    "                critic_target_net_dict[key] = self.tau * critic_net_dict[key] + (1-self.tau) * critic_target_net_dict[key]\n",
    "            self.critic_target_net.load_state_dict(critic_target_net_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(3, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, states):\n",
    "        x = self.relu(self.fc1(states))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.tanh(self.fc3(x))\n",
    "    \n",
    "    \n",
    "class RandomActor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def computeActions(self, states, target=False, deterministic=True):\n",
    "        if target or not deterministic:\n",
    "            raise ValueError(\"Random actor does not have a target network or noise\")\n",
    "\n",
    "        # generate random actions between -1 and 1\n",
    "        actions = torch.rand((states.shape[0],1))\n",
    "        return 2*actions - 1\n",
    "    \n",
    "\n",
    "class HeuristicActor():\n",
    "    def __init__(self, const_torque):\n",
    "        if const_torque > 1 or const_torque < 0:\n",
    "            raise ValueError(\"Constant torque must be between 0 and 1\")\n",
    "        self.const_torque = const_torque\n",
    "\n",
    "        self.log_losses = []\n",
    "    \n",
    "    def saveModels(self, path):\n",
    "        pass\n",
    "\n",
    "    def computeActions(self, states, target=False, deterministic=True):\n",
    "        # if target or not deterministic:\n",
    "        #     raise ValueError(\"Heuristic actor does not have a target network or noise\")\n",
    "        \n",
    "        # generate heuristic actions\n",
    "        actions = torch.empty((states.shape[0], 1))\n",
    "        actions[:,0] = -torch.sign(states[:,0]) * torch.sign(states[:,2]) * self.const_torque\n",
    "        return actions\n",
    "    \n",
    "    def trainStep(self, batch, critic):\n",
    "        self.log_losses.append(0)\n",
    "\n",
    "\n",
    "class Actor():\n",
    "    def __init__(self, lr, tau, noise:ActionNoise):\n",
    "        # hyperparameters\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "\n",
    "        self.noise = noise\n",
    "        \n",
    "        # initialize actor networks\n",
    "        self.actor_net = ActorNetwork()\n",
    "\n",
    "        # intialize critic and actor optimizers\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_net.parameters(), lr=self.lr)\n",
    "\n",
    "        # initialize target networks\n",
    "        self.actor_target_net = ActorNetwork()\n",
    "\n",
    "        # logging values\n",
    "        self.log_losses = []\n",
    "\n",
    "    def saveModels(self, path):\n",
    "        torch.save(self.actor_net, os.path.join(path, \"actor_net.pt\"))\n",
    "        torch.save(self.actor_target_net, os.path.join(path, \"actor_target_net.pt\"))\n",
    "\n",
    "    def computeActions(self, states, target=False, deterministic=True):\n",
    "        if target:\n",
    "            actions = self.actor_target_net.forward(states=states)\n",
    "        else:\n",
    "            actions = self.actor_net.forward(states=states)\n",
    "\n",
    "        if not deterministic:\n",
    "            actions += self.noise.getNoisyAction(actions=actions)\n",
    "        return actions\n",
    "\n",
    "    def trainStep(self, batch:dict, critic):\n",
    "        # do not train if relay buffer is not large enough\n",
    "        if batch is None:\n",
    "            self.log_losses.append(0)\n",
    "            return\n",
    "\n",
    "        # freeze critic network to avoid unnecessary computations of gradients\n",
    "        for p in critic.critic_net.parameters():\n",
    "            p.requires_grad = False        \n",
    "\n",
    "        # gradient descent step for actor network\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss = self._computeActorLoss(batch=batch, critic=critic)\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.log_losses.append(actor_loss.item())\n",
    "\n",
    "        # unfreeze critic network\n",
    "        for p in critic.critic_net.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # update target network\n",
    "        self._updateTargetNetworks()\n",
    "    \n",
    "    def _computeActorLoss(self, batch, critic):\n",
    "        # estimate action from state\n",
    "        actions = self.computeActions(states=batch['state'], target=False, deterministic=True)\n",
    "\n",
    "        # calculate q values for state-action pairs\n",
    "        q_values = critic.computeQValues(states=batch['state'], actions=actions, target=False)\n",
    "\n",
    "        # calculate loss      \n",
    "        return - q_values.mean()\n",
    "    \n",
    "    def _updateTargetNetworks(self):\n",
    "        if self.tau == 1.0:\n",
    "            return\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # update actor target network\n",
    "            actor_net_dict = self.actor_net.state_dict()\n",
    "            actor_target_net_dict = self.actor_target_net.state_dict()\n",
    "            for key in actor_target_net_dict:\n",
    "                actor_target_net_dict[key] = self.tau * actor_net_dict[key] + (1-self.tau) * actor_target_net_dict[key]\n",
    "            self.actor_target_net.load_state_dict(actor_target_net_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env:NormalizedEnv, \n",
    "            critic:Critic,\n",
    "            actor:Actor,\n",
    "            buffer:ReplayBuffer,\n",
    "        ) -> None:\n",
    "\n",
    "        self.env = env\n",
    "        self.critic = critic\n",
    "        self.actor = actor\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def run(self, num_episodes, render, plot):\n",
    "        # create figure for rendering\n",
    "        if render:\n",
    "            fig = plt.figure()\n",
    "            frames = []\n",
    "\n",
    "        # run episodes\n",
    "        step_rewards = []\n",
    "        cum_rewards = []\n",
    "        for i in range(num_episodes):\n",
    "            # print testing progress\n",
    "            if i%10 == 0:\n",
    "                print(f\"Testing episode: {i}/{num_episodes}\")\n",
    "\n",
    "            # reset environment\n",
    "            state = self.buffer.numpy2tensor(self.env.reset()[0], shape_type=\"state\") # tuple contains as first element the state\n",
    "            while True:\n",
    "                # take action and update environment\n",
    "                action = self.actor.computeActions(states=state, target=False, deterministic=True)\n",
    "                next_state, reward, term, trunc, info = self.env.step(action=action.detach().numpy().flatten()) # TODO: action has wrond dimensions\n",
    "                state = self.buffer.numpy2tensor(next_state, shape_type=\"state\")\n",
    "\n",
    "                # log reward\n",
    "                step_rewards.append(reward)\n",
    "\n",
    "                # render environment\n",
    "                if render:\n",
    "                    env_screen = self.env.render()\n",
    "                    frames.append([plt.imshow(env_screen)])       \n",
    "\n",
    "                # check if episode is truncated\n",
    "                if trunc:\n",
    "                    assert len(step_rewards) % 200 == 0 # verfiy that episode length is 200\n",
    "                    break\n",
    "\n",
    "            # log cummulative reward\n",
    "            cum_rewards.append(np.sum(step_rewards[i*200:]))\n",
    "\n",
    "        # show animation of environment\n",
    "        if render:\n",
    "            ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True, repeat_delay=1000)\n",
    "            plt.show()\n",
    "\n",
    "        # plot rewards\n",
    "        if plot:\n",
    "            self._plotReward(step_rewards=step_rewards, cum_rewards=cum_rewards, title=\"reward_testing\")\n",
    "\n",
    "        return step_rewards, cum_rewards\n",
    "    \n",
    "    def train(self, num_episodes, batch_size):\n",
    "        # plot heat maps before training\n",
    "        self._plotHeatmap(title=\"heatmap_before_training\")\n",
    "        self._plotPolarHeatMap(title=\"polar_heatmap_before_training\")\n",
    "\n",
    "        # run episodes\n",
    "        step_rewards = []\n",
    "        cum_rewards = []\n",
    "        for i in range(num_episodes):\n",
    "            # print training progress\n",
    "            if i%10 == 0:\n",
    "                print(f\"Training episode: {i}/{num_episodes}\")\n",
    "\n",
    "            # reset environment\n",
    "            state = self.buffer.numpy2tensor(self.env.reset()[0], shape_type=\"state\") # tuple contains as first element the state\n",
    "            if hasattr(self.actor, \"noise\"):\n",
    "                self.actor.noise.reset()\n",
    "            while True:\n",
    "                # take action and update environment\n",
    "                action = self.actor.computeActions(states=state, target=False, deterministic=False)\n",
    "                next_state, reward, term, trunc, info = self.env.step(action=action.detach().numpy().flatten())\n",
    "\n",
    "                # add transition to replay buffer\n",
    "                self.buffer.addTransition(state=state, action=action, reward=reward, next_state=next_state, trunc=trunc)\n",
    "                state = self.buffer.numpy2tensor(next_state, shape_type=\"state\")\n",
    "\n",
    "                # train Q and policy networks if replay buffer is large enough\n",
    "                batch = self.buffer.sampleBatch(batch_size=batch_size)\n",
    "                self.critic.trainStep(batch=self.buffer.detachClone(batch), actor=self.actor)\n",
    "                self.actor.trainStep(batch=self.buffer.detachClone(batch), critic=self.critic)\n",
    "\n",
    "                # log reward\n",
    "                step_rewards.append(reward)\n",
    "\n",
    "                # check if episode is truncated\n",
    "                if trunc:\n",
    "                    assert len(step_rewards) % 200 == 0 # verfiy that episode length is 200\n",
    "                    break\n",
    "\n",
    "            # log cummulative reward\n",
    "            cum_rewards.append(np.sum(step_rewards[i*200:]))\n",
    "\n",
    "        # plot rewards, losses and heat maps\n",
    "        self._plotReward(step_rewards=step_rewards, cum_rewards=cum_rewards, title=\"reward_training\")    \n",
    "        self._plotLosses(critic_losses=self.critic.log_losses, actor_losses=self.actor.log_losses)\n",
    "        self._plotPolarHeatMap()\n",
    "\n",
    "        return step_rewards, cum_rewards\n",
    "\n",
    "    def _plotReward(self, step_rewards, cum_rewards, title=\"reward\"):\n",
    "        # assure that episode length is 200\n",
    "        assert len(step_rewards) % 200 == 0\n",
    "\n",
    "        # average losses of one episode\n",
    "        episode_mean = []\n",
    "        episode_per5 = []\n",
    "        episode_per95 = []\n",
    "        i = 0\n",
    "        while i < len(step_rewards):\n",
    "            episodes_rewards = np.array(step_rewards[i:i+200])\n",
    "            episode_mean.append(episodes_rewards.mean())\n",
    "            episode_per5.append(np.percentile(episodes_rewards, 5))\n",
    "            episode_per95.append(np.percentile(episodes_rewards, 95))\n",
    "            i += 200\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 10))\n",
    "        fig.suptitle(\"Rewards per episode\")\n",
    "\n",
    "        axs[0].plot(range(len(episode_mean)), cum_rewards, label=\"Cummulative\", color=\"red\")\n",
    "        axs[0].set_xlabel(\"Episode\")\n",
    "        axs[0].set_ylabel(\"Reward\")\n",
    "        axs[0].legend()\n",
    "        axs[0].set_title(f\"Cummulative reward per episode (avg={np.round(np.mean(cum_rewards), 3)})\")\n",
    "\n",
    "        axs[1].plot(range(len(episode_mean)), episode_mean, label=\"Mean\", color=\"red\")\n",
    "        axs[1].fill_between(x=range(len(episode_mean)), y1=episode_per5, y2=episode_per95, alpha=0.2, color=\"blue\", label=\"Percentile 5-95%\")\n",
    "        axs[1].set_xlabel(\"Episode\")\n",
    "        axs[1].set_ylabel(\"Reward\")\n",
    "        axs[1].legend()\n",
    "        axs[1].set_title(f\"Mean reward per episode (avg={np.round(np.mean(episode_mean), 3)})\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def _plotLosses(self, critic_losses, actor_losses):\n",
    "        # assure that episode length is 200\n",
    "        assert len(critic_losses) % 200 == 0 and len(critic_losses) == len(actor_losses)\n",
    "        \n",
    "        c_losses = []\n",
    "        a_losses = []\n",
    "        i = 0\n",
    "        while i < len(critic_losses):\n",
    "            c_losses.append(np.mean(critic_losses[i:i+200]))\n",
    "            a_losses.append(np.mean(actor_losses[i:i+200]))\n",
    "            i += 200\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.plot(c_losses, label=\"Critic Loss\", color=\"green\")\n",
    "        plt.plot(a_losses, label=\"Actor Loss\", color=\"blue\")\n",
    "        plt.title(\"Losses\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def _plotPolarHeatMap(self, title=\"polar_heatmap\"):\n",
    "        res_angle = 360\n",
    "        res_radial = 10\n",
    "\n",
    "        radius = np.linspace(0, 1, res_radial)\n",
    "        angle = np.linspace(-np.pi, np.pi, res_angle)\n",
    "\n",
    "        r, a = np.meshgrid(radius, angle)\n",
    "        cos_a = torch.cos(torch.tensor(a, dtype=torch.float32)).reshape(-1,1)\n",
    "        sin_a = torch.sin(torch.tensor(a, dtype=torch.float32)).reshape(-1,1)   \n",
    "\n",
    "        velocities = [0, 2.5]\n",
    "        torques = [-1, 0, 1]\n",
    "        q_values = []\n",
    "        for i, v in enumerate(velocities):\n",
    "            for j, torque in enumerate(torques):\n",
    "                vel = v * torch.ones_like(cos_a, dtype=torch.float32)\n",
    "                states = torch.concat((cos_a, sin_a, vel), axis=1)\n",
    "                actions = torque * torch.ones_like(cos_a, dtype=torch.float32)\n",
    "                q_val = self.critic.computeQValues(states=states, actions=actions.reshape(-1,1), target=False)\n",
    "\n",
    "                q_val = q_val.detach().numpy().reshape(res_angle, res_radial)\n",
    "                q_values.append(q_val)\n",
    "                \n",
    "        \n",
    "        q_val_max = np.max(q_values)\n",
    "        q_val_min = np.min(q_values)\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12,7), subplot_kw={'projection':\"polar\"})      \n",
    "        for idx, q_val in enumerate(q_values):\n",
    "            i = idx // len(torques)\n",
    "            j = idx % len(torques)\n",
    "                    \n",
    "            cb = axs[i,j].pcolormesh(a, r, q_val, vmin=q_val_min, vmax=q_val_max)\n",
    "            axs[i,j].plot(angle, r, color='k', ls='none') \n",
    "            fig.colorbar(cb, ax=axs[i,j])\n",
    "            axs[i,j].set_yticks([],[])\n",
    "            axs[i,j].set_theta_offset(np.pi/2)\n",
    "            axs[i,j].set_theta_direction(-1)\n",
    "            axs[i,j].set_title(f\"Torque={2*torques[j]}Nm, vel={v}m/s\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3 - Heuristic Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and actor\n",
    "env = NormalizedEnv(env=gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"))\n",
    "critic = None\n",
    "buffer = ReplayBuffer(buffer_size=100000, seed=1)\n",
    "actor = RandomActor()\n",
    "\n",
    "# run algorithm\n",
    "simu = Simulation(\n",
    "    env=env, \n",
    "    critic = critic,\n",
    "    actor = actor, \n",
    "    buffer=buffer,\n",
    ")\n",
    "step_rewards, cum_rewards = simu.run(num_episodes=10, render=False, plot=True)\n",
    "\n",
    "print(f\"Mean cummulative reward: {np.mean(cum_rewards)}, std: {np.std(cum_rewards)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torques = np.linspace(0, 1, 11)\n",
    "\n",
    "# create environment and actor\n",
    "env = NormalizedEnv(env=gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"))\n",
    "critic = None\n",
    "buffer = ReplayBuffer(buffer_size=100000, seed=1)\n",
    "\n",
    "cum_sums = []\n",
    "cum_stds = []\n",
    "step_sums = []\n",
    "step_stds = []\n",
    "for torque in torques:\n",
    "    # create actor\n",
    "    actor = HeuristicActor(const_torque=torque)\n",
    "\n",
    "    # run algorithm\n",
    "    simu = Simulation(\n",
    "        env=env, \n",
    "        critic = critic,\n",
    "        actor = actor, \n",
    "        buffer=buffer,\n",
    "    )\n",
    "    step_rewards, cum_rewards = simu.run(num_episodes=10, render=False, plot=False)\n",
    "\n",
    "    # save mean and std of cummulative and step rewards\n",
    "    cum_sums.append(np.mean(cum_rewards))\n",
    "    cum_stds.append(np.std(cum_rewards))\n",
    "    step_sums.append(np.mean(step_rewards))\n",
    "    step_stds.append(np.std(step_rewards))\n",
    "\n",
    "# transform torques in action space [0, 1] to torque space [0, 2]\n",
    "torques = torques * 2\n",
    "\n",
    "# plot results\n",
    "fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(10, 5))\n",
    "axs[0].errorbar(x=torques, y=cum_sums, yerr=cum_stds, ecolor=\"red\", label=\"Mean and std\")\n",
    "axs[0].set_xlabel(\"Constant Torque\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Cummulative reward\")\n",
    "\n",
    "axs[1].errorbar(x=torques, y=step_sums, yerr=step_stds, ecolor=\"red\", label=\"Mean and std\")\n",
    "axs[1].set_xlabel(\"Constant Torque\")\n",
    "axs[1].set_ylabel(\"Reward\")\n",
    "axs[1].legend()\n",
    "axs[1].set_title(\"Average reward\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 4 - Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment, critic, actor, noise and buffer\n",
    "env = NormalizedEnv(env=gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"))\n",
    "critic = Critic(gamma=0.99, lr=1e-4, tau=1.0)\n",
    "actor = HeuristicActor(const_torque=1.0)\n",
    "buffer = ReplayBuffer(buffer_size=10000, seed=1)\n",
    "\n",
    "# train algorithm\n",
    "simu = Simulation(\n",
    "    env=env, \n",
    "    critic = critic,\n",
    "    actor = actor, \n",
    "    buffer=buffer,\n",
    ")\n",
    "simu.train(num_episodes=1000, batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 5 - Minimal Implementation of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment, critic, actor, noise and buffer\n",
    "env = NormalizedEnv(env=gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"))\n",
    "critic = Critic(gamma=0.99, lr=1e-4, tau=1.0)\n",
    "action_noise = GaussianActionNoise(sigma=0.3, seed=0)\n",
    "actor = Actor(lr=1e-4, tau=1.0, noise=action_noise)\n",
    "buffer = ReplayBuffer(buffer_size=100000, seed=1)\n",
    "\n",
    "# train algorithm\n",
    "simu = Simulation(\n",
    "    env=env, \n",
    "    critic = critic,\n",
    "    actor = actor, \n",
    "    buffer=buffer,\n",
    ")\n",
    "simu.train(num_episodes=1000, batch_size=128)\n",
    "simu.run(num_episodes=100, render=False, plot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6 - Target DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target network update\n",
    "tau = 0.01\n",
    "\n",
    "# create environment, critic, actor, noise and buffer\n",
    "env = NormalizedEnv(env=gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"))\n",
    "critic = Critic(gamma=0.99, lr=1e-4, tau=tau)\n",
    "action_noise = GaussianActionNoise(sigma=0.3, seed=0)\n",
    "actor = Actor(lr=1e-4, tau=tau, noise=action_noise)\n",
    "buffer = ReplayBuffer(buffer_size=100000, seed=1)\n",
    "\n",
    "# train algorithm\n",
    "simu = Simulation( \n",
    "    env=env, \n",
    "    critic = critic,\n",
    "    actor = actor, \n",
    "    buffer=buffer,\n",
    ")\n",
    "simu.train(num_episodes=1000, batch_size=128)\n",
    "simu.run(num_episodes=100, render=False, plot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 7 - Ornstein-Uhlenbeck Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU noise parameter\n",
    "theta = 0.0\n",
    "\n",
    "# create environment, critic, actor, noise and buffer\n",
    "env = NormalizedEnv(env=gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"))\n",
    "critic = Critic(gamma=0.99, lr=1e-4, tau=0.01)\n",
    "action_noise = OUActionNoise(sigma=0.3, theta=theta, seed=0)\n",
    "actor = Actor(lr=1e-4, tau=0.01, noise=action_noise)\n",
    "buffer = ReplayBuffer(buffer_size=100000, seed=1)\n",
    "\n",
    "# train algorithm\n",
    "simu = Simulation(\n",
    "    env=env, \n",
    "    critic = critic,\n",
    "    actor = actor, \n",
    "    buffer=buffer,\n",
    ")\n",
    "simu.train(num_episodes=1000, batch_size=128)\n",
    "simu.run(num_episodes=100, render=False, plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
